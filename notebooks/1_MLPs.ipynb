{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical 1: Multi-Layer Perceptrons\n",
    "\n",
    "**Open notebook:** \n",
    "[![View on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/phlippe/asci_cbl_practicals/blob/main/notebooks/1_MLPs.ipynb)\n",
    "[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/phlippe/asci_cbl_practicals/blob/main/notebooks/1_MLPs.ipynb)    \n",
    "**Authors:** Phillip Lippe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this practical, you will get a first experience in working with images and neural networks by implementing multi-layer perceptrons (MLPs). You will experiment with different optimization and regularization strategies to get the best performance out of your model, and investigate the limitations of using MLPs for image processing. This notebook is intended to guide you through the practical and you can edit it in any place. If you prefer working with standard python scripts, feel free to convert this notebook into a python script. To open this notebook on Google Colab, use the button above. Note that you need to copy this notebook into your own Google Drive to save the notebook and trained models. Otherwise, your progress will be lost when you close the browser tab.\n",
    "\n",
    "First of all, let's start with importing some standard libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Standard libraries\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import numpy as np \n",
    "import copy\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "## Progress bar\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "\n",
    "## PyTorch Torchvision\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout the practicals, we make use of several datasets and might want to save different models. For this, we define the two paths below. Adjust the paths as you like, but remember to keep them consistent across practicals to not download the dataset several times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the folder where the datasets are/should be downloaded (e.g. MNIST)\n",
    "DATASET_PATH = \"../data\"\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = \"../saved_models/practical1\"\n",
    "os.makedirs(CHECKPOINT_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training deep neural networks is a stochastic process. Hence, every time you run the notebook, you naturally get different results. To obtain reproducible results, it is highly recommended to set a seed for all stochastic operations. Hence, let's define a `set_seed` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for setting the seed\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "set_seed(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.determinstic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Fetching the device that will be used throughout this notebook\n",
    "device = torch.device(\"cpu\") if not torch.cuda.is_available() else torch.device(\"cuda:0\")\n",
    "print(\"Using device\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's start with setting up the dataset we will use in this practical: CIFAR10. CIFAR10 is a very popular dataset for computer vision on low-resolution images (32x32 pixels). The task is to classify images into one of 10 classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck. Our goal is to develop a neural network that can solve this task efficiently and accurately. To load the dataset, we can luckily make use of PyTorch's library `torchvision` which provides access to many popular vision datasets and more practical functions. So, let's load the dataset below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset statistics for normalizing the input values to zero mean and one std\n",
    "DATA_MEANS = [0.491, 0.482, 0.447]\n",
    "DATA_STD = [0.247, 0.243, 0.261]\n",
    "\n",
    "# Transformations are applied on images when we want to access them. Here, we push the images into a tensor\n",
    "# and normalize the values. However, you can use more transformations, like augmentations to prevent overfitting.\n",
    "# Feel free to experiment with augmentations here once you have a first running MLP, but remember to not apply\n",
    "# any augmentations on the test data!\n",
    "data_transforms = transforms.Compose([transforms.ToTensor(),\n",
    "                                      transforms.Normalize(DATA_MEANS, DATA_STD)\n",
    "                                     ])\n",
    "\n",
    "# Loading the training dataset. We need to split it into a training and validation part\n",
    "main_dataset = CIFAR10(root=DATASET_PATH, train=True, transform=data_transforms, download=True)\n",
    "train_set, val_set = torch.utils.data.random_split(main_dataset, [45000, 5000], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "# Loading the test set\n",
    "test_set = CIFAR10(root=DATASET_PATH, train=False, transform=data_transforms, download=True)\n",
    "\n",
    "# Create data loaders for later\n",
    "train_loader = data.DataLoader(train_set, batch_size=128, shuffle=True, drop_last=True, pin_memory=True, num_workers=3)\n",
    "val_loader = data.DataLoader(val_set, batch_size=128, shuffle=False, drop_last=False, num_workers=3)\n",
    "test_loader = data.DataLoader(test_set, batch_size=128, shuffle=False, drop_last=False, num_workers=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with data, it is always recommend to look at the data before blaming your model for not performing well if the data was incorrectly processed. Hence, let's plot the first 10 images of the CIFAR10 training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_IMAGES = 10\n",
    "images = [train_set[idx][0] for idx in range(NUM_IMAGES)]\n",
    "img_grid = torchvision.utils.make_grid(torch.stack(images, dim=0), nrow=5, normalize=True, pad_value=0.5)\n",
    "img_grid = img_grid.permute(1, 2, 0)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.title(\"Dataset examples from CIFAR10\", fontsize=20)\n",
    "plt.imshow(img_grid)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are all set. So, let's dive into implementing our own MLP!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Implementing the MLP\n",
    "\n",
    "As a first step, let's implement a simple MLP. \n",
    "\n",
    "### MLP Module\n",
    "\n",
    "You can make use of PyTorch's common functionalities, especially the `torch.nn` modules might be of help. The design choices of the MLP (e.g. the activation function) is left up to you, but for an initial setup, we recommend stacking linear layers with ReLU activation functions in between. Remember to not apply any activation function on the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size=3072, num_classes=10, hidden_sizes=[256, 128]):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            input_size - Size of the input images in pixels\n",
    "            num_classes - Number of classes we want to predict. The output size of the MLP\n",
    "                          should be num_classes.\n",
    "            hidden_sizes - A list of integers specifying the hidden layer sizes in the MLP. \n",
    "                           The MLP should have len(hidden_sizes)+1 linear layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # TODO: Create the network based on the specified hidden sizes\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # TODO: Apply the MLP on an input \n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test the MLP implementation\n",
    "input_size = np.random.randint(low=64, high=3072)\n",
    "num_classes = np.random.randint(low=5, high=20)\n",
    "hidden_sizes = [np.random.randint(low=32, high=256) for _ in range(np.random.randint(low=1, high=3))]\n",
    "my_mlp = MLP(input_size=input_size, num_classes=num_classes, hidden_sizes=hidden_sizes)\n",
    "my_mlp.to(device)\n",
    "random_input = torch.randn(32, input_size, device=device)\n",
    "random_output = my_mlp(random_input)\n",
    "assert random_output.shape[0] == random_input.shape[0]\n",
    "assert random_output.shape[1] == num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "\n",
    "To gain a better insight in the training of the neural networks, let's implement our own optimizer. First, we need to understand what an optimizer actually does. The optimizer is responsible to update the network's parameters given the gradients. Hence, we effectively implement a function $w^{t} = f(w^{t-1}, g^{t}, ...)$ with $w$ being the parameters, and $g^{t} = \\nabla_{w^{(t-1)}} \\mathcal{L}^{(t)}$ the gradients at time step $t$. A common, additional parameter to this function is the learning rate, here denoted by $\\eta$. Usually, the learning rate can be seen as the \"step size\" of the update. A higher learning rate means that we change the weights more in the direction of the gradients, a smaller means we take shorter steps. \n",
    "\n",
    "As most optimizers only differ in the implementation of $f$, we can define a template for an optimizer in PyTorch below. We take as input the parameters of a model and a learning rate. The function `zero_grad` sets the gradients of all parameters to zero, which we have to do before calling `loss.backward()`. Finally, the `step()` function tells the optimizer to update all weights based on their gradients. The template is setup below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizerTemplate:\n",
    "    \n",
    "    def __init__(self, params, lr):\n",
    "        self.params = list(params)\n",
    "        self.lr = lr\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        ## Set gradients of all parameters to zero\n",
    "        for p in self.params:\n",
    "            if p.grad is not None:\n",
    "                p.grad.detach_() # For second-order optimizers important\n",
    "                p.grad.zero_()\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        ## Apply update step to all parameters\n",
    "        for p in self.params:\n",
    "            if p.grad is None: # We skip parameters without any gradients\n",
    "                continue\n",
    "            self.update_param(p)\n",
    "            \n",
    "    def update_param(self, p):\n",
    "        # To be implemented in optimizer-specific classes\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimizer we are going to implement is the standard Stochastic Gradient Descent (SGD) with momentum. Plain SGD updates the parameters using the following equation:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "    w^{(t)} & = w^{(t-1)} - \\eta \\cdot g^{(t)}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "The concept of momentum replaces the gradient in the update by an exponential average of all past gradients including the current one, which allows for a smoother training. The gradient update with momentum becomes:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "    m^{(t)} & = \\beta_1 m^{(t-1)} + (1 - \\beta_1)\\cdot g^{(t)}\\\\\n",
    "    w^{(t)} & = w^{(t-1)} - \\eta \\cdot m^{(t)}\\\\\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Let's implement the optimizer below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDMomentum(OptimizerTemplate):\n",
    "    \n",
    "    def __init__(self, params, lr, momentum=0.9):\n",
    "        super().__init__(params, lr)\n",
    "        self.momentum = momentum # Corresponds to beta_1 in the equation above\n",
    "        self.param_momentum = {p: torch.zeros_like(p.data) for p in self.params} # Dict to store m_t\n",
    "        \n",
    "    def update_param(self, p):\n",
    "        # TODO: Implement the gradient update\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify that our optimizer is working, let's create a challenging surface over two parameter dimensions which we want to optimize to find the optimum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pathological_curve_loss(w1, w2):\n",
    "    # Example of a pathological curvature. There are many more possible, feel free to experiment here!\n",
    "    x1_loss = torch.tanh(w1)**2 + 0.01 * torch.abs(w1)\n",
    "    x2_loss = torch.sigmoid(w2)\n",
    "    return x1_loss + x2_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curve(curve_fn, x_range=(-5,5), y_range=(-5,5), plot_3d=False, cmap=cm.viridis, title=\"Pathological curvature\"):\n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    if plot_3d:\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "    else:\n",
    "        ax = fig.gca()\n",
    "    \n",
    "    x = torch.arange(x_range[0], x_range[1], (x_range[1]-x_range[0])/100.)\n",
    "    y = torch.arange(y_range[0], y_range[1], (y_range[1]-y_range[0])/100.)\n",
    "    x, y = torch.meshgrid([x,y], indexing='ij')\n",
    "    z = curve_fn(x, y)\n",
    "    x, y, z = x.numpy(), y.numpy(), z.numpy()\n",
    "    \n",
    "    if plot_3d:\n",
    "        ax.plot_surface(x, y, z, cmap=cmap, linewidth=1, color=\"#000\", antialiased=False)\n",
    "        ax.set_zlabel(\"loss\")\n",
    "    else:\n",
    "        ax.imshow(z.T[::-1], cmap=cmap, extent=(x_range[0], x_range[1], y_range[0], y_range[1]))\n",
    "    plt.title(title)\n",
    "    ax.set_xlabel(r\"$w_1$\")\n",
    "    ax.set_ylabel(r\"$w_2$\")\n",
    "    plt.tight_layout()\n",
    "    return ax\n",
    "\n",
    "sns.reset_orig()\n",
    "_ = plot_curve(pathological_curve_loss, plot_3d=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of optimization, you can image that $w_1$ and $w_2$ are weight parameters, and the curvature represents the loss surface over the space of $w_1$ and $w_2$. Note that in typical networks, we have many, many more parameters than two, and such curvatures can occur in multi-dimensional spaces as well.\n",
    "\n",
    "Ideally, our optimization algorithm would find the center of the ravine and focuses on optimizing the parameters towards the direction of $w_2$. However, if we encounter a point along the ridges, the gradient is much greater in $w_1$ than $w_2$, and we might end up jumping from one side to the other. Due to the large gradients, we would have to reduce our learning rate slowing down learning significantly.\n",
    "\n",
    "To test our algorithms, we can implement a simple function to train two parameters on such a surface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_curve(optimizer_func, curve_func=pathological_curve_loss, num_updates=100, init=[5,5]):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        optimizer_func - Constructor of the optimizer to use. Should only take a parameter list\n",
    "        curve_func - Loss function (e.g. pathological curvature)\n",
    "        num_updates - Number of updates/steps to take when optimizing \n",
    "        init - Initial values of parameters. Must be a list/tuple with two elements representing w_1 and w_2\n",
    "    Outputs:\n",
    "        Numpy array of shape [num_updates, 3] with [t,:2] being the parameter values at step t, and [t,2] the loss at t.\n",
    "    \"\"\"\n",
    "    weights = nn.Parameter(torch.FloatTensor(init), requires_grad=True)\n",
    "    optimizer = optimizer_func([weights])\n",
    "    \n",
    "    list_points = []\n",
    "    for _ in range(num_updates):\n",
    "        # TODO: Determine the loss for the current weights, save the weights and loss, perform backpropagation\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    points = torch.stack(list_points, dim=0).numpy()\n",
    "    return points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's apply the optimizer on our curvature. Note that we set a much higher learning rate for the optimization algorithms as you would in a standard neural network. This is because we only have 2 parameters instead of tens of thousands or even millions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SGDMom_points = train_curve(lambda params: SGDMomentum(params, lr=10, momentum=0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand best how the different algorithms worked, we visualize the update step as a line plot through the loss surface. We will stick with a 2D representation for readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_points = SGDMom_points\n",
    "ax = plot_curve(pathological_curve_loss,\n",
    "                x_range=(-np.absolute(all_points[:,0]).max(), np.absolute(all_points[:,0]).max()),\n",
    "                y_range=(all_points[:,1].min(), all_points[:,1].max()),\n",
    "                plot_3d=False)\n",
    "ax.plot(SGDMom_points[:,0], SGDMom_points[:,1], color=\"red\", marker=\"o\", zorder=2, label=\"SGDMom\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the implementation is correct, you should see that the optimizer indeed reaches a point of very low $w_2$ ($w_2 < -7.5$) and $w_1\\approx 0$. If not, go back to your optimizer implementation and check what could go wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and validation function\n",
    "\n",
    "Now that we the MLP ready, the optimizer implemented, and the dataset loaded, we can look at implementing our own training function. The form of the training function is left mostly up to you, but we recommend that you test the model on the validation function after every 5 epochs, and save the best model. We provide a rough template below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, train_data_loader, val_data_loader, num_epochs=25, model_name=\"MyMLP\"):\n",
    "    # Set model to train mode\n",
    "    model.to(device)\n",
    "    best_val_acc = -1.0\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        model.train()\n",
    "        for imgs, labels in train_data_loader:\n",
    "            # TODO: Implement training loop with training on classification\n",
    "            raise NotImplementedError\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            # TODO: Evaluate the model and save if best\n",
    "            raise NotImplementedError\n",
    "    \n",
    "    # Load best model after training\n",
    "    load_model(model, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test_model(model, data_loader):\n",
    "    # TODO: Test model and return accuracy\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, model_name):\n",
    "    # TODO: Save the parameters of the model\n",
    "    raise NotImplementedError\n",
    "    \n",
    "def load_model(model, model_name):\n",
    "    # TODO: Load the parameters of the model\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create model, optimizer, and start training\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test best model on test set\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is expected that you reach with the default configurations a validation and test accuracy of $\\sim52-53\\%$ . If you have reached this performance, we can consider this task as completed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Improving your MLP\n",
    "\n",
    "Now that we have a basic MLP, let's try to improve over this default performance! Your task is to think about ways to maximize the performance of your MLP. Possible suggestions you can look at include:\n",
    " \n",
    "* Do data augmentations help the model to generalize?\n",
    "* Can regularization techniques (dropout, weight decay, etc.) help?\n",
    "* Do deeper models perform better? Or is it better to have wide networks?\n",
    "* Can normalization techniques (BatchNorm, LayerNorm, etc.) help?\n",
    "\n",
    "Your task is to improve your model to reach at least 56% on the test set! But can you get even above this? Consider this as a challenge! \n",
    "\n",
    "For this implementation, you can directly edit your model above. In your report, list the changes that you have made and discuss what affect they have. Further, repeat the experiment for *at least 3 seeds* to report stable improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Improve the model\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Investigating the limitations\n",
    "\n",
    "Now that we have seen how we can optimize our MLP, it is good to investigate the limitations of the model as well. Images have a natural grid structure where close-by pixels are strongly related. Does the MLP make use of this structure? To investigate this question, we will run two experiments:\n",
    " \n",
    "* Create a shuffle of pixels at the beginning of the training, and use the same shuffle throughout the training and validation.\n",
    "* At each training and validation step, sample a new shuffle of pixels.\n",
    " \n",
    " \n",
    "It is up to you whether you perform this investigation on the original plain MLP version or your optimized one. Implement a corresponding train and test function that support both variants of shuffling, and train two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a training and test function that supports the shuffling of pixels, both fixed and newly generated for each batch\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create model, optimizer, and start training on fixed shuffling of pixels\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create model, optimizer, and start training on a new shuffling of pixels per batch/image\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What results do you observe? What does this tell us about the MLP being aware of the image structure? Add your results and observations to your report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You have reached the end of the practical, congratulations! Now, you should have a good idea of what it means to train a MLP, how we can use neural networks to perform image classification, and what tricks there are to improve a networks performance. In the next practicals, we will look at more advanced network structures beyond MLPs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
